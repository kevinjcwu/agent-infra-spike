# Repository Structure - Visual Guide

**Last Updated**: November 10, 2025
**Status**: âœ… Current Architecture (Post-Refactoring)

This guide shows the actual current structure after the November 10, 2025 refactoring.

---

## ğŸ“ Repository Overview

```
agent-infra-spike/
â”‚
â”œâ”€â”€ ğŸ“‚ orchestrator/                    # MAF-based conversational orchestrator
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ orchestrator_agent.py           # Multi-turn conversation manager
â”‚   â”œâ”€â”€ tool_manager.py                 # Dynamic tool registration (@decorator pattern)
â”‚   â”œâ”€â”€ capability_registry.py          # Anti-hallucination validation
â”‚   â”œâ”€â”€ tools.py                        # 4 tools: select, suggest, estimate, execute
â”‚   â””â”€â”€ models.py                       # ConversationState, ProvisioningPlan
â”‚
â”œâ”€â”€ ğŸ“‚ capabilities/                    # Pluggable infrastructure capabilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                         # BaseCapability interface
â”‚   â”œâ”€â”€ README.md                       # How to add new capabilities
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ databricks/                  # â­ Databricks capability (3-layer architecture)
â”‚       â”œâ”€â”€ __init__.py                 # Public API exports
â”‚       â”œâ”€â”€ capability.py               # Main DatabricksCapability class
â”‚       â”œâ”€â”€ README.md                   # Databricks capability documentation
â”‚       â”‚
â”‚       â”œâ”€â”€ ğŸ“‚ core/                    # ğŸ”µ Business Logic Layer
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ config.py               # Databricks config (instance types, pricing)
â”‚       â”‚   â”œâ”€â”€ intent_parser.py        # NL â†’ InfrastructureRequest (Azure OpenAI)
â”‚       â”‚   â””â”€â”€ decision_maker.py       # Configuration decisions (GPU/CPU, SKU)
â”‚       â”‚
â”‚       â”œâ”€â”€ ğŸ“‚ models/                  # ğŸŸ¢ Data Models Layer
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ schemas.py              # Pydantic models (Request, Decision, Result)
â”‚       â”‚
â”‚       â””â”€â”€ ğŸ“‚ provisioning/            # ğŸŸ¡ Infrastructure Layer
â”‚           â”œâ”€â”€ __init__.py
â”‚           â””â”€â”€ ğŸ“‚ terraform/
â”‚               â”œâ”€â”€ __init__.py
â”‚               â”œâ”€â”€ generator.py        # Terraform HCL generation (Jinja2)
â”‚               â””â”€â”€ executor.py         # Terraform execution (subprocess)
â”‚
â”œâ”€â”€ ğŸ“‚ templates/                       # Terraform Jinja2 templates
â”‚   â”œâ”€â”€ main.tf.j2                      # Resource definitions
â”‚   â”œâ”€â”€ variables.tf.j2                 # Variable declarations
â”‚   â”œâ”€â”€ outputs.tf.j2                   # Output definitions
â”‚   â”œâ”€â”€ provider.tf.j2                  # Provider config
â”‚   â””â”€â”€ terraform.tfvars.j2             # Variable values
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                           # Test suite (94 tests)
â”‚   â”œâ”€â”€ test_maf_setup.py               # MAF integration tests
â”‚   â”œâ”€â”€ test_orchestrator.py            # Orchestrator tests
â”‚   â”œâ”€â”€ test_capability_integration.py  # End-to-end capability tests
â”‚   â”œâ”€â”€ test_config.py                  # Config tests
â”‚   â”œâ”€â”€ test_decision_maker.py          # Decision making tests
â”‚   â”œâ”€â”€ test_models.py                  # Data model tests
â”‚   â”œâ”€â”€ test_terraform_generator.py     # Terraform generation tests
â”‚   â””â”€â”€ test_terraform_executor.py      # Terraform execution tests
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                            # Documentation
â”‚   â”œâ”€â”€ PRD.md                          # Product requirements
â”‚   â”œâ”€â”€ ARCHITECTURE_EVOLUTION.md       # Design decisions & roadmap
â”‚   â”œâ”€â”€ DATABRICKS_REFACTORING_SUMMARY.md  # Current refactoring details
â”‚   â”œâ”€â”€ MAF_RESEARCH_AND_IMPLEMENTATION_PLAN.md  # MAF integration
â”‚   â”œâ”€â”€ MAF_TOOL_CALLING_FIX.md         # Technical fix documentation
â”‚   â”œâ”€â”€ visual_structure.md             # This file
â”‚   â””â”€â”€ ğŸ“‚ implementation_status/       # Phase-by-phase history
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ PHASE_0_RESULTS.md
â”‚       â”œâ”€â”€ PHASE_1_RESULTS.md
â”‚       â”œâ”€â”€ PHASE_1.5_TOOL_REGISTRY.md
â”‚       â”œâ”€â”€ PHASE_1.6_CAPABILITY_REGISTRY.md
â”‚       â””â”€â”€ PHASE_2_CAPABILITY_INTEGRATION.md
â”‚
â”œâ”€â”€ ğŸ“‚ .github/
â”‚   â””â”€â”€ copilot-instructions.md         # â­ Primary coding guidelines
â”‚
â”œâ”€â”€ cli_maf.py                          # ğŸ¯ Conversational CLI (main entry point)
â”œâ”€â”€ README.md                           # Project overview
â”œâ”€â”€ CURRENT_STATE.md                    # Current project status
â”œâ”€â”€ pyproject.toml                      # Dependencies
â””â”€â”€ .env                                # Azure credentials (not in git)
```

---

## ğŸ—ï¸ Three-Layer Architecture (Databricks Capability)

### ğŸ”µ Core Layer (`capabilities/databricks/core/`)
**Purpose**: Business logic and decision-making

**Files**:
- **`config.py`** - Databricks configuration
  - Instance type definitions (CPU: ds4_v2, ds5_v2 / GPU: nc6s_v3, nc12s_v3)
  - Pricing data ($0.20-3.06/hour)
  - Azure region mappings
  - Spark versions, workload size mappings

- **`intent_parser.py`** - Natural language parsing
  - Uses Azure OpenAI GPT-4o with function calling
  - Extracts: team, environment, region, workload_type
  - Returns: `InfrastructureRequest` (Pydantic model)

- **`decision_maker.py`** - Infrastructure decisions
  - Selects instance types based on workload
  - Chooses Databricks SKU (Trial/Standard/Premium)
  - Calculates costs and enforces limits
  - Returns: `InfrastructureDecision` (Pydantic model)

**No IaC coupling** - Can switch from Terraform to Bicep without touching this layer

### ğŸŸ¢ Models Layer (`capabilities/databricks/models/`)
**Purpose**: Type-safe data structures

**Files**:
- **`schemas.py`** - All Pydantic data classes
  - `InfrastructureRequest` - User requirements
  - `InfrastructureDecision` - Selected configuration + costs
  - `TerraformFiles` - Generated HCL files
  - `DeploymentResult` - Deployment outputs + metadata

**Pure data** - No business logic, no external dependencies (except Pydantic)

### ğŸŸ¡ Provisioning Layer (`capabilities/databricks/provisioning/terraform/`)
**Purpose**: Infrastructure-as-Code implementation

**Files**:
- **`generator.py`** - Terraform HCL generation
  - Renders Jinja2 templates from `templates/*.tf.j2`
  - Produces 5 files: main.tf, variables.tf, outputs.tf, provider.tf, terraform.tfvars

- **`executor.py`** - Terraform execution
  - Runs: `terraform init`, `plan`, `apply`, `destroy`
  - Parses JSON outputs
  - Manages working directory and state

**IaC-specific** - Could have sibling `provisioning/bicep/` in future

---

## ğŸ”„ Data Flow

### User Request â†’ Deployed Infrastructure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  USER: "I need Databricks for ML team in production"        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLI (cli_maf.py)                                            â”‚
â”‚  â€¢ Captures user input                                       â”‚
â”‚  â€¢ Calls orchestrator.process_message()                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ORCHESTRATOR (orchestrator/orchestrator_agent.py)           â”‚
â”‚  â€¢ Multi-turn conversation (MAF manages context)             â”‚
â”‚  â€¢ Uses tools:                                               â”‚
â”‚    - select_capabilities (validates capability names)        â”‚
â”‚    - suggest_naming (Azure naming conventions)               â”‚
â”‚    - estimate_cost (monthly cost breakdown)                  â”‚
â”‚    - execute_deployment (triggers deployment)                â”‚
â”‚  â€¢ Detects execute_deployment in LLM response                â”‚
â”‚  â€¢ Calls: capability.plan() â†’ capability.execute()           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CAPABILITY (capabilities/databricks/capability.py)          â”‚
â”‚                                                              â”‚
â”‚  plan() Phase:                                               â”‚
â”‚    1. Parse parameters from context                          â”‚
â”‚    2. IntentParser: NL â†’ InfrastructureRequest               â”‚
â”‚    3. DecisionMaker: Request â†’ InfrastructureDecision        â”‚
â”‚    4. TerraformGenerator: Decision â†’ TerraformFiles          â”‚
â”‚    5. TerraformExecutor: Run terraform plan (dry-run)        â”‚
â”‚    6. Return CapabilityPlan (resources, costs, files)        â”‚
â”‚                                                              â”‚
â”‚  execute() Phase:                                            â”‚
â”‚    1. Reconstruct TerraformFiles from plan                   â”‚
â”‚    2. TerraformExecutor: Run terraform apply                 â”‚
â”‚    3. Parse outputs (workspace_url, workspace_id)            â”‚
â”‚    4. Return CapabilityResult                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AZURE RESOURCES (Deployed via Terraform)                    â”‚
â”‚  â€¢ Resource Group: rg-ml-team-prod                           â”‚
â”‚  â€¢ Databricks Workspace: ml-prod                             â”‚
â”‚    URL: https://adb-xxxx.azuredatabricks.net                 â”‚
â”‚  â€¢ Databricks Cluster: ml-prod-cluster (GPU/CPU instances)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Key Technologies

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Agent Framework** | Microsoft Agent Framework (MAF) 1.0.0b251105 | Conversation management |
| **LLM** | Azure OpenAI GPT-4o | Intent parsing, function calling |
| **Data Validation** | Pydantic 2.5+ | Type-safe data models |
| **Templating** | Jinja2 | Terraform HCL generation |
| **IaC** | Terraform 1.5+ | Infrastructure provisioning |
| **Cloud Providers** | azurerm ~3.80, databricks ~1.29 | Azure + Databricks resources |
| **Testing** | pytest + pytest-asyncio | Test suite (94 tests) |

---

## ğŸ“Š Implementation Status

### âœ… Completed Phases

**Phase 0**: MAF Integration (6 tests)
- Microsoft Agent Framework setup
- Azure OpenAI connectivity

**Phase 1**: Conversational Orchestrator (9 tests)
- Multi-turn conversation
- Parameter gathering

**Phase 1.5**: Tool Registry Pattern
- Dynamic tool registration
- Auto-schema generation

**Phase 1.6**: Capability Registry
- Anti-hallucination validation
- Capability name validation

**Phase 2**: Capability Integration (8 tests)
- BaseCapability interface
- DatabricksCapability with 3-layer architecture

**November 10 Refactoring**: Architecture cleanup
- Removed legacy agent/ directory (741 lines)
- Organized databricks into 3 layers
- Renamed classes for clarity (IntentParser, DecisionMaker)
- All 94 tests passing

### ğŸš€ What Works Today

âœ… **Conversational Interface**: Multi-turn dialogue with parameter gathering
âœ… **Tool-Enabled**: 4 working tools for capability discovery, naming, cost estimation, execution
âœ… **Actual Deployment**: Deploys to Azure in ~13 minutes
âœ… **Verified**: Working Databricks workspace URL in production
âœ… **Test Coverage**: 94/94 tests passing (100%)

### ğŸ¯ Future Phases (Post-Spike)

**Phase 3**: State Persistence & Robustness
- Persistent conversation state
- Resume interrupted deployments
- Comprehensive error handling

**Phase 4**: Second Capability
- Azure OpenAI provisioning
- Multi-capability workflows

**Phase 5**: Enterprise Features
- RBAC, cost budgets, approval workflows
- Monitoring, alerting, integrations

---

## ğŸ“ Key Patterns

### 1. Tool Registry Pattern
Dynamic tool registration with decorators:
```python
@tool_manager.register("Tool description")
def my_tool(param: str) -> str:
    return result
```
- Auto-generates OpenAI function schemas
- Scales to 100+ tools without code changes

### 2. Capability Registry Pattern
Prevents LLM hallucination:
```python
capability_registry.register(
    name="provision_databricks",
    description="Provision Databricks workspace",
    tags=["azure", "databricks"]
)
```
- LLM provides semantic understanding
- Registry validates capability names
- Rejects hallucinated capabilities

### 3. Three-Layer Architecture
Clear separation of concerns:
- **Core**: What decisions do we make?
- **Models**: What data do we work with?
- **Provisioning**: How do we deploy it?

### 4. Public API Pattern
All exports through `__init__.py`:
```python
from capabilities.databricks import (
    DatabricksCapability,
    IntentParser,
    DecisionMaker,
    Config
)
```

---

## ğŸ§ª Testing

### Test Organization
```
tests/
â”œâ”€â”€ test_maf_setup.py              # 6 tests - MAF integration
â”œâ”€â”€ test_orchestrator.py           # 9 tests - Orchestrator logic
â”œâ”€â”€ test_capability_integration.py # 8 tests - End-to-end capability
â”œâ”€â”€ test_config.py                 # 12 tests - Configuration
â”œâ”€â”€ test_decision_maker.py         # 11 tests - Decision logic
â”œâ”€â”€ test_models.py                 # 9 tests - Data models
â”œâ”€â”€ test_terraform_generator.py    # 21 tests - HCL generation
â””â”€â”€ test_terraform_executor.py     # 18 tests - Terraform execution
```

### Run Tests
```bash
# All tests
pytest tests/ -v

# Specific layer
pytest tests/test_decision_maker.py -v

# With coverage
pytest tests/ --cov=orchestrator --cov=capabilities
```

**Current Status**: 94/94 tests passing âœ…

---

## ğŸ“ Learning Resources

### For New Contributors

**Start Here**:
1. `README.md` - Project overview
2. `.github/copilot-instructions.md` - Coding guidelines (PRIMARY)
3. This file - Structure understanding
4. `capabilities/databricks/README.md` - Example capability

**Deep Dives**:
- `docs/PRD.md` - Original requirements
- `docs/ARCHITECTURE_EVOLUTION.md` - Design decisions & roadmap
- `docs/DATABRICKS_REFACTORING_SUMMARY.md` - Recent refactoring details
- `docs/MAF_TOOL_CALLING_FIX.md` - Technical insights

**Adding Features**:
- `capabilities/README.md` - How to add new capabilities
- `docs/implementation_status/` - Phase-by-phase history

---

## ğŸ’¡ Design Principles

### Separation of Concerns
Each module has a single responsibility - orchestrator routes, capabilities provision, layers separate business logic from infrastructure code.

### Scalability
- Tool Registry: Add tools without touching routing code
- Capability Registry: Validate capabilities without hardcoding
- Three-Layer Architecture: Template for all future capabilities

### Type Safety
Pydantic models throughout for validation and IDE support.

### Testability
Each layer independently testable with clear interfaces.

### No Premature Extraction
Keep code in capabilities until duplication appears across 2-3 capabilities, then extract to `shared/`.

---

## ğŸ“ Entry Points

**Main CLI**: `cli_maf.py` - Conversational interface

**Key Classes**:
- `orchestrator.InfrastructureOrchestrator` - Main orchestrator
- `capabilities.databricks.DatabricksCapability` - Databricks provisioning
- `capabilities.base.BaseCapability` - Capability interface

**Configuration**: `.env` file with Azure credentials

---

**Last Updated**: November 10, 2025
**Maintainer**: GitHub Copilot + Human
**Status**: âœ… Production-ready architecture with 94/94 tests passing
