Metadata-Version: 2.4
Name: agent-infra-spike
Version: 0.1.0
Summary: AI-powered agent that automates Databricks workspace provisioning from natural language requests
Author: Infrastructure Automation Team
License: MIT
Keywords: databricks,terraform,llm,infrastructure,automation,azure
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: agent-framework>=1.0.0b251105
Requires-Dist: openai>=1.3.0
Requires-Dist: pydantic>=2.5.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: jinja2>=3.1.2
Requires-Dist: click>=8.1.7
Requires-Dist: azure-identity>=1.15.0
Requires-Dist: azure-mgmt-resource>=23.0.0
Requires-Dist: requests>=2.31.0
Requires-Dist: pyyaml>=6.0.1
Provides-Extra: dev
Requires-Dist: pytest>=7.4.3; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: pytest-mock>=3.12.0; extra == "dev"
Requires-Dist: black>=23.12.0; extra == "dev"
Requires-Dist: mypy>=1.7.0; extra == "dev"
Requires-Dist: ruff>=0.1.9; extra == "dev"
Requires-Dist: types-requests>=2.31.0; extra == "dev"
Requires-Dist: types-pyyaml>=6.0.12; extra == "dev"

# Infrastructure Provisioning Agent - Spike/POC

AI-powered conversational orchestrator that automates infrastructure provisioning through multi-turn dialogue.

## ğŸ‰ Status: Spike Complete & Deployed

**Successfully deployed to Azure on November 7, 2025**
- âœ… Conversational orchestrator working end-to-end
- âœ… Verified Azure deployment: [Databricks workspace](https://adb-4412170593674511.11.azuredatabricks.net)
- âœ… Resource Group: `rg-e2e-deploy-dev`
- âœ… Deployment time: ~13 minutes (785 seconds)

---

## ğŸš€ Evolution Journey

**From**: Single-purpose Databricks agent with single-shot commands
**To**: Multi-capability orchestration platform with conversational interface

**Completed Phases**:
- âœ… **Phase 0**: Microsoft Agent Framework (MAF) integration with Azure OpenAI
- âœ… **Phase 1**: Multi-turn conversational orchestrator with tool integration
- âœ… **Phase 1.5**: Tool Registry pattern for scalable tool management
- âœ… **Phase 1.6**: Capability Registry pattern for hallucination prevention
- âœ… **Phase 2**: Capability integration with BaseCapability interface
- âœ… **Actual Azure Deployment**: Verified working infrastructure

**Current Scope**:
- ğŸ¯ **Databricks Deployment** (production-ready)
  - Azure Resource Group
  - Azure Databricks Workspace (Premium SKU)
  - Databricks Compute Cluster (GPU/CPU with autoscaling)
  - ~13 minutes from conversation to deployed infrastructure

---

## ğŸ“š Documentation Navigation

**For Developers**:
- [`.github/copilot-instructions.md`](.github/copilot-instructions.md) - Primary coding guidance

**Architecture & Design**:
- [`docs/ARCHITECTURE_EVOLUTION.md`](docs/ARCHITECTURE_EVOLUTION.md) - Vision, decisions, next phases
- [`docs/STRUCTURE_VISUAL_GUIDE.md`](docs/STRUCTURE_VISUAL_GUIDE.md) - Current structure with diagrams
- [`docs/PRD.md`](docs/PRD.md) - Original product requirements

**Implementation**:
- [`capabilities/README.md`](capabilities/README.md) - How to add new capabilities
- [`docs/implementation_status/`](docs/implementation_status/) - Phase-by-phase progress
- [`docs/MAF_TOOL_CALLING_FIX.md`](docs/MAF_TOOL_CALLING_FIX.md) - Technical fix details

---

## ğŸ¯ What This Does

Reduces Databricks workspace provisioning from **3-4 hours (manual)** to **~13 minutes (automated)**.

### Key Features

**Conversational Interface**:
- Multi-turn dialogue to gather requirements
- Smart defaults with user customization
- Natural language parameter extraction
- Cost estimation before deployment

**Tool-Enabled Orchestrator**:
- `select_capabilities` - Validates infrastructure capability names
- `suggest_naming` - Generates Azure-compliant resource names
- `estimate_cost` - Calculates monthly cost breakdown
- `execute_deployment` - Triggers actual deployment to Azure

**Pluggable Architecture**:
- BaseCapability interface for all infrastructure types
- DatabricksCapability wraps proven deployment logic
- Easy to extend with new capabilities (OpenAI, Firewall, etc.)

---

## ğŸ›ï¸ Architecture Overview

### Conversational Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER: "I need Databricks for ML team"                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ORCHESTRATOR (Multi-turn Conversation)                    â”‚
â”‚  â€¢ Asks clarifying questions (team name, environment, region)               â”‚
â”‚  â€¢ Uses tools to suggest names, estimate costs, validate capabilities       â”‚
â”‚  â€¢ Proposes deployment plan with cost breakdown                             â”‚
â”‚  â€¢ Detects execute_deployment tool call and triggers deployment             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CAPABILITY (DatabricksCapability)                         â”‚
â”‚  â€¢ plan(): Parse requirements, make decisions, generate Terraform           â”‚
â”‚  â€¢ execute(): Run terraform apply, return workspace URL                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DATABRICKS MODULES (3-Layer Architecture)                 â”‚
â”‚  Core Layer:                                                                â”‚
â”‚  â€¢ IntentParser: Natural language â†’ InfrastructureRequest                   â”‚
â”‚  â€¢ DecisionMaker: Configuration decisions (GPU/CPU, SKU, sizing)            â”‚
â”‚  Models Layer:                                                              â”‚
â”‚  â€¢ Schemas: Pydantic data classes for type safety                           â”‚
â”‚  Provisioning Layer:                                                        â”‚
â”‚  â€¢ TerraformGenerator: Jinja2 templates â†’ HCL files                         â”‚
â”‚  â€¢ TerraformExecutor: Run terraform init/plan/apply                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          AZURE RESOURCES âœ…                                 â”‚
â”‚  â€¢ Resource Group: rg-e2e-deploy-dev                                        â”‚
â”‚  â€¢ Databricks Workspace: e2e-deploy-dev                                     â”‚
â”‚  â€¢ Databricks Cluster: e2e-deploy-dev-cluster (GPU instances)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

**Orchestrator Layer** (`orchestrator/`):
- MAF-based conversational agent
- Tool registry with dynamic registration
- Capability registry for validation
- Multi-turn conversation management

**Capability Layer** (`capabilities/`):
- BaseCapability interface (plan/validate/execute/rollback)
- DatabricksCapability with 3-layer architecture:
  - Core: Business logic (IntentParser, DecisionMaker, Config)
  - Models: Data structures (Pydantic schemas)
  - Provisioning: Infrastructure deployment (Terraform)
- Pluggable architecture for new infrastructure types

**See**: [`docs/STRUCTURE_VISUAL_GUIDE.md`](docs/STRUCTURE_VISUAL_GUIDE.md) for detailed diagrams

---
â”‚  â”‚    - outputs.tf (output values)                                    â”‚     â”‚
â”‚  â”‚    - terraform.tfvars (variable values)                            â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                      TerraformFiles
                      {
                        provider.tf: "terraform {...}"
                        main.tf: "resource \"azurerm_resource_group\" {...}"
                        variables.tf: "variable \"workspace_name\" {...}"
                        outputs.tf: "output \"workspace_url\" {...}"
                        terraform.tfvars: "workspace_name = \"ml-prod\""
                      }
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   USER APPROVAL GATE     â”‚
                    â”‚  (unless --auto-approve) â”‚
                    â”‚                          â”‚
                    â”‚  Show Terraform plan     â”‚
                    â”‚  Estimated cost          â”‚
                    â”‚  Resources to create     â”‚
                    â”‚                          â”‚
                    â”‚  [Y/n] to proceed        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        4. TERRAFORM EXECUTOR                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Subprocess Management for Terraform                               â”‚     â”‚
â”‚  â”‚  1. Write files to working directory                               â”‚     â”‚
â”‚  â”‚  2. Run: terraform init                                            â”‚     â”‚
â”‚  â”‚  3. Run: terraform plan                                            â”‚     â”‚
â”‚  â”‚  4. Run: terraform apply (if approved)                             â”‚     â”‚
â”‚  â”‚  5. Parse outputs (workspace URL, IDs, etc.)                       â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    AZURE DEPLOYMENT      â”‚
                    â”‚  (via Terraform)         â”‚
                    â”‚                          â”‚
                    â”‚  â€¢ Resource Group        â”‚
                    â”‚  â€¢ Databricks Workspace  â”‚
                    â”‚  â€¢ Instance Pool         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            DEPLOYMENT RESULT                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  âœ… Success!                                                       â”‚     â”‚
â”‚  â”‚                                                                    â”‚     â”‚
â”‚  â”‚  Workspace URL: https://adb-123456.azuredatabricks.net             â”‚     â”‚
â”‚  â”‚  Resource Group: rg-ml-prod                                        â”‚     â”‚
â”‚  â”‚  Instance Pool ID: 1234-567890-pool-abc123                         â”‚     â”‚
â”‚  â”‚  Deployment Time: 13m 2s                                           â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Details

#### 1. **Intent Parser** (`capabilities/databricks/core/intent_parser.py`)
- **Technology**: Azure OpenAI GPT-4o with Function Calling
- **Purpose**: Converts natural language to structured data
- **How it works**:
  - Defines JSON schema with required fields (team, environment, region)
  - GPT-4 reads user message + schema
  - LLM extracts values using semantic understanding (no regex!)
  - Returns validated `InfrastructureRequest` object
- **Example**:
  - Input: `"Create dev workspace for test team in East US"`
  - Output: `{"team": "test", "environment": "dev", "region": "eastus", ...}`

#### 2. **Decision Maker** (`capabilities/databricks/core/decision_maker.py`)
- **Technology**: Python business logic
- **Purpose**: Makes intelligent infrastructure configuration decisions
- **How it works**:
  - Maps workload types to instance sizes
  - GPU workloads â†’ GPU instances (NC-series)
  - Production â†’ larger clusters, premium SKU
  - Development â†’ smaller clusters, standard SKU
  - Calculates cost estimates from Azure pricing
- **Example**:
  - ML + Prod â†’ `Standard_NC6s_v3`, Premium SKU, 2-8 workers
  - Data Engineering + Dev â†’ `Standard_D4s_v5`, Standard SKU, 1-2 workers

#### 3. **Terraform Generator** (`capabilities/databricks/provisioning/terraform/generator.py`)
- **Technology**: Jinja2 templating engine
- **Purpose**: Generates production-ready Terraform HCL files
- **How it works**:
  - Loads templates from `capabilities/databricks/templates/*.j2`
  - Renders with decision variables
  - Produces 5 Terraform files ready for execution
- **Templates**:
  - `provider.tf.j2` â†’ Azure & Databricks provider config
  - `main.tf.j2` â†’ Resource definitions (RG, workspace, cluster)
  - `variables.tf.j2` â†’ Input variable declarations
  - `outputs.tf.j2` â†’ Output value definitions
  - `terraform.tfvars.j2` â†’ Variable value assignments

#### 4. **Terraform Executor** (`capabilities/databricks/provisioning/terraform/executor.py`)
- **Technology**: Python subprocess management
- **Purpose**: Executes Terraform commands and manages deployment lifecycle
- **How it works**:
  - Writes Terraform files to working directory
  - Runs `terraform init` (downloads providers)
  - Runs `terraform plan` (shows what will be created)
  - Waits for approval (interactive or auto)
  - Runs `terraform apply` (provisions resources)
  - Parses outputs (workspace URL, IDs)
  - Returns `DeploymentResult` with status and metadata

### Data Flow Summary

```
Natural Language
    â†’ InfrastructureRequest (parsed)
        â†’ InfrastructureDecision (configured)
            â†’ TerraformFiles (generated)
                â†’ Azure Resources (deployed)
                    â†’ DeploymentResult (returned)
```

### Key Technologies

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **NLP Parsing** | Azure OpenAI GPT-4o Function Calling | Structured data extraction |
| **Template Engine** | Jinja2 | HCL file generation |
| **IaC Execution** | Terraform 1.13+ | Infrastructure provisioning |
| **Cloud Providers** | azurerm ~3.80, databricks ~1.29 | Azure + Databricks resources |
| **Authentication** | Azure CLI (`az login`) | Both providers use azure-cli auth |
| **CLI Framework** | Click 8.3 | User-friendly command interface |

> **Note**: This implementation uses **OpenAI Function Calling** directly via the Azure OpenAI API for structured data extraction. It does **NOT** use Microsoft Agent Framework (MAF), Semantic Kernel, or AutoGen. We chose direct OpenAI API integration for simplicity and direct control over the function calling schema. For this single-agent, one-shot parsing use case, the additional abstraction layers of MAF/Semantic Kernel would be unnecessary complexity.

### Resources Deployed

Each successful deployment creates:
1. **Azure Resource Group** (azurerm_resource_group)
2. **Databricks Workspace** (azurerm_databricks_workspace) - Standard or Premium SKU
3. **Databricks Instance Pool** (databricks_instance_pool) - Pre-warmed compute VMs

## ğŸš€ Quick Start

### Prerequisites
- Python 3.12+
- Azure subscription with Databricks permissions
- Azure OpenAI service with GPT-4o deployment

### Setup

```bash
# 1. Clone repository
git clone https://github.com/YOUR_ORG/agent-infra-spike.git
cd agent-infra-spike

# 2. Set up environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -e .

# 3. Configure credentials
cp .env.example .env
# Edit .env with your Azure OpenAI and Azure credentials:
#   AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
#   AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o
#   AZURE_OPENAI_API_VERSION=2024-08-01-preview
#   AZURE_SUBSCRIPTION_ID=your-subscription-id
#   AZURE_TENANT_ID=your-tenant-id
```

### Run Conversational Interface

```bash
# Start the conversational orchestrator
python cli_maf.py

# Example interaction:
# You: "I need a Databricks workspace for ML experimentation"
# Agent: "I'll help you provision a Databricks workspace. Let me ask a few questions..."
# Agent: "What environment? (dev/staging/prod)"
# You: "dev"
# Agent: "Which Azure region? (eastus/westus2/centralus)"
# You: "eastus"
# Agent: "I'll create these resources:
#        - Resource Group: rg-ml-dev
#        - Workspace: ml-dev
#        - Cluster: ml-dev-cluster
#        Estimated cost: ~$1,600/month
#        Proceed with deployment?"
# You: "yes"
# Agent: [Deploys to Azure in ~13 minutes]
# Agent: "âœ… Deployment complete! Workspace URL: https://adb-xxxx.azuredatabricks.net"
```

### Run Tests

```bash
# All tests (23 tests)
pytest tests/ -v

# By phase
pytest tests/test_maf_setup.py -v              # Phase 0 (6 tests)
pytest tests/test_orchestrator.py -v           # Phase 1 (9 tests)
pytest tests/test_capability_integration.py -v # Phase 2 (8 tests)

# With coverage
pytest tests/ --cov=orchestrator --cov=capabilities --cov=agent --cov-report=html
```

---

## ğŸ—ï¸ Project Structure

```
agent-infra-spike/
â”œâ”€â”€ orchestrator/                    # MAF-based conversational orchestrator
â”‚   â”œâ”€â”€ orchestrator_agent.py        # Main conversation manager
â”‚   â”œâ”€â”€ tool_manager.py              # Dynamic tool registration
â”‚   â”œâ”€â”€ tools.py                     # 4 tools (select, suggest, estimate, execute)
â”‚   â”œâ”€â”€ capability_registry.py       # Anti-hallucination validation
â”‚   â””â”€â”€ models.py                    # Data models
â”‚
â”œâ”€â”€ capabilities/                    # Pluggable infrastructure capabilities
â”‚   â”œâ”€â”€ base.py                      # BaseCapability interface
â”‚   â””â”€â”€ databricks/                  # Databricks capability (3-layer architecture)
â”‚       â”œâ”€â”€ capability.py            # Main orchestrator
â”‚       â”œâ”€â”€ README.md                # Capability documentation
â”‚       â”œâ”€â”€ core/                    # Business Logic Layer
â”‚       â”‚   â”œâ”€â”€ config.py            # Configuration & pricing
â”‚       â”‚   â”œâ”€â”€ intent_parser.py     # LLM: NL â†’ InfrastructureRequest
â”‚       â”‚   â””â”€â”€ decision_maker.py    # Configuration decisions
â”‚       â”œâ”€â”€ models/                  # Data Models Layer
â”‚       â”‚   â””â”€â”€ schemas.py           # Pydantic data classes
      â”œâ”€â”€ provisioning/            # Infrastructure Layer
       â”‚   â””â”€â”€ terraform/
       â”‚       â”œâ”€â”€ generator.py     # Jinja2: Decision â†’ HCL
       â”‚       â””â”€â”€ executor.py      # Terraform CLI wrapper
       â””â”€â”€ templates/               # Terraform Jinja2 templates
           â”œâ”€â”€ main.tf.j2
           â”œâ”€â”€ variables.tf.j2
           â”œâ”€â”€ outputs.tf.j2
           â”œâ”€â”€ provider.tf.j2
           â””â”€â”€ terraform.tfvars.j2
â”‚
â”œâ”€â”€ tests/                           # Test suite (94 tests, all passing)
â”‚   â”œâ”€â”€ test_maf_setup.py            # Phase 0
â”‚   â”œâ”€â”€ test_orchestrator.py         # Phase 1
â”‚   â”œâ”€â”€ test_capability_integration.py # Phase 2
â”‚   â”œâ”€â”€ test_decision_maker.py       # Decision making tests
â”‚   â”œâ”€â”€ test_terraform_generator.py  # Terraform generation tests
â”‚   â””â”€â”€ test_terraform_executor.py   # Terraform execution tests
â”‚
â”œâ”€â”€ docs/                            # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE_EVOLUTION.md    # Vision and next phases
â”‚   â”œâ”€â”€ STRUCTURE_VISUAL_GUIDE.md    # Current structure
â”‚   â”œâ”€â”€ PRD.md                       # Original requirements
â”‚   â””â”€â”€ implementation_status/       # Phase-by-phase progress
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ copilot-instructions.md      # Primary coding guidance
â”‚
â”œâ”€â”€ cli_maf.py                       # ğŸ¯ Conversational CLI (USE THIS)
â”œâ”€â”€ pyproject.toml                   # Python dependencies
â””â”€â”€ .env                             # Azure credentials
```

**See**: [`docs/STRUCTURE_VISUAL_GUIDE.md`](docs/STRUCTURE_VISUAL_GUIDE.md) for detailed architecture diagrams

---

## ğŸ“Š Implementation Status

### âœ… Completed (Spike Complete)

**Phase 0: MAF Integration**
- Microsoft Agent Framework v2025-03-01-preview
- Azure OpenAI connectivity validated
- 6 tests passing

**Phase 1: Conversational Orchestrator**
- Multi-turn conversation with parameter gathering
- MAF automatic context management
- 9 tests passing

**Phase 1.5: Tool Registry Pattern**
- Dynamic tool registration with `@tool_manager.register`
- Auto-schema generation from type hints
- 4 tools implemented and working

**Phase 1.6: Capability Registry**
- Anti-hallucination validation
- LLM semantic understanding + registry validation
- Prevents invalid capability names

**Phase 2: Capability Integration**
- BaseCapability interface (plan/validate/execute/rollback)
- DatabricksCapability with layered architecture (core/models/provisioning)
- **Actual Azure deployment verified**
- 8 tests passing

**Deployment Proof**:
- âœ… Workspace URL: `https://adb-4412170593674511.11.azuredatabricks.net`
- âœ… Resource Group: `rg-e2e-deploy-dev`
- âœ… Duration: ~13 minutes (785 seconds)
- âœ… Resources: RG + Workspace + Cluster (GPU instances)

### ğŸ¯ What's Next (Post-Spike)

**Phase 3: State Persistence & Robustness** (2-3 weeks)
- File-based or database-backed conversation state
- Resume interrupted deployments
- Comprehensive error handling with rollback
- Audit logging for all operations

**Phase 4: Second Capability** (2-3 weeks)
- Add Azure OpenAI provisioning capability
- Test multi-capability workflows
- Implement capability dependency management
- Partial success handling

**Phase 5: Enterprise Features** (4-6 weeks)
- Role-based access control
- Cost budgets and approval workflows
- Monitoring and alerting integration
- Jira/ServiceNow ticket integration
- Self-service portal or Slack/Teams bot interface

**See**: [`docs/ARCHITECTURE_EVOLUTION.md`](docs/ARCHITECTURE_EVOLUTION.md) for detailed roadmap

---

## ğŸ§ª Development

```bash
# Run tests
pytest tests/ -v

# Format code
black orchestrator/ capabilities/ tests/

# Type check
mypy orchestrator/ capabilities/

# Lint
ruff check orchestrator/ capabilities/
```

---

## ğŸ“ Learning Resources

**Key Files to Understand the System**:
1. `.github/copilot-instructions.md` - Comprehensive coding guidance
2. `docs/STRUCTURE_VISUAL_GUIDE.md` - Architecture diagrams and data flow
3. `docs/ARCHITECTURE_EVOLUTION.md` - Design decisions and future plans
4. `capabilities/README.md` - How to add new capabilities
5. `orchestrator/orchestrator_agent.py` - Main orchestrator implementation
6. `capabilities/databricks/capability.py` - Reference capability implementation

**Important Concepts**:
- **Three-Layer Architecture**: Core (logic) / Models (data) / Provisioning (infrastructure)
- **capabilities/databricks/**: Self-contained with all deployment logic
- **Tool Registry Pattern**: Dynamic registration, scales to 100+ tools
- **Capability Registry**: Prevents LLM hallucination of invalid capabilities
- **MAF Tool Calling**: Must pass actual functions, not JSON schemas

---

## ğŸ“ License

[Your License]
